{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":407317,"sourceType":"datasetVersion","datasetId":181273}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-21T10:27:04.033795Z","iopub.execute_input":"2024-05-21T10:27:04.034205Z","iopub.status.idle":"2024-05-21T10:27:09.511809Z","shell.execute_reply.started":"2024-05-21T10:27:04.034173Z","shell.execute_reply":"2024-05-21T10:27:09.510540Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/engine.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/utils.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/transforms.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/coco_eval.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/coco_utils.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch_snippets import *\nfrom IPython import display\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom torchinfo import summary\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, models, datasets\nfrom xml.etree import ElementTree as et\nimport torchvision\nimport glob\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.patches as patches\nimport tqdm\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:10:24.390264Z","iopub.execute_input":"2024-05-21T15:10:24.390703Z","iopub.status.idle":"2024-05-21T15:10:24.400135Z","shell.execute_reply.started":"2024-05-21T15:10:24.390673Z","shell.execute_reply":"2024-05-21T15:10:24.398761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torchvision.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:31:48.607168Z","iopub.execute_input":"2024-05-21T14:31:48.607704Z","iopub.status.idle":"2024-05-21T14:31:48.615393Z","shell.execute_reply.started":"2024-05-21T14:31:48.607669Z","shell.execute_reply":"2024-05-21T14:31:48.613685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/lgg-mri-segmentation/kaggle_3m/data.csv')#.head(4)\nprint(f'Количество уникальных пациентов: {df.Patient.nunique()}')\nprint('-----------------------------------------------------------')\ndf.head(4)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:35:38.400919Z","iopub.execute_input":"2024-05-21T10:35:38.401829Z","iopub.status.idle":"2024-05-21T10:35:38.441186Z","shell.execute_reply.started":"2024-05-21T10:35:38.401776Z","shell.execute_reply":"2024-05-21T10:35:38.439548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Создаем списки путей картинок и масок**","metadata":{}},{"cell_type":"code","source":"brain_scans = []\nmask_files = glob.glob('../input/lgg-mri-segmentation/kaggle_3m/*/*_mask*')\n\nfor i in mask_files:\n    brain_scans.append(i.replace('_mask',''))\n\nprint(brain_scans[:2])\nprint(mask_files[:2])\nprint(len(brain_scans))\nprint(len(mask_files))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:35:38.444272Z","iopub.execute_input":"2024-05-21T10:35:38.445036Z","iopub.status.idle":"2024-05-21T10:35:38.614085Z","shell.execute_reply.started":"2024-05-21T10:35:38.444989Z","shell.execute_reply":"2024-05-21T10:35:38.612941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------\n-------------------------------------------------\n-------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# **2. Передварительный анализ**","metadata":{}},{"cell_type":"code","source":"#Lets plot some samples\nrows,cols=3,3\nfig=plt.figure(figsize=(10,10))\nfor i in range(1,rows*cols+1):\n    fig.add_subplot(rows,cols,i)\n    img_path=brain_scans[i]\n    msk_path=mask_files[i]\n    img=cv2.imread(img_path)\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    msk=cv2.imread(msk_path)\n    plt.imshow(img)\n    plt.imshow(msk,alpha=0.3)\n    plt.xticks([]), plt.yticks([])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:35:40.958840Z","iopub.execute_input":"2024-05-21T10:35:40.960348Z","iopub.status.idle":"2024-05-21T10:35:42.445256Z","shell.execute_reply.started":"2024-05-21T10:35:40.960304Z","shell.execute_reply":"2024-05-21T10:35:42.443751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Смотрим содержимое каналов масок**","metadata":{}},{"cell_type":"code","source":"rows,cols=1,3\nfig=plt.figure(figsize=(10,10))\nfor i in np.arange(1,4):\n    fig.add_subplot(rows,cols,i)\n    msk_path=mask_files[2]\n    msk=cv2.imread(msk_path)\n    plt.imshow(msk.transpose(-1,0,1)[i-1],alpha=0.3)\n    plt.xticks([]), plt.yticks([])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:45:31.266186Z","iopub.execute_input":"2024-05-21T11:45:31.267124Z","iopub.status.idle":"2024-05-21T11:45:31.523510Z","shell.execute_reply.started":"2024-05-21T11:45:31.267088Z","shell.execute_reply":"2024-05-21T11:45:31.521914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# читаем картинку маски, 4меняем каналы местасми на (3,250,250),\nan = cv2.imread(mask_files[2], 1).transpose(2,0,1)#/255\nr,g,b = an\n\n# координаты пикселей маски\nnzs = np.nonzero(r==255)#==255)\ninstances = np.unique(g[nzs])\nnzs","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:45:45.264125Z","iopub.execute_input":"2024-05-21T11:45:45.264881Z","iopub.status.idle":"2024-05-21T11:45:45.278058Z","shell.execute_reply.started":"2024-05-21T11:45:45.264845Z","shell.execute_reply":"2024-05-21T11:45:45.276743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Уникальные значения в каждом канале:**","metadata":{}},{"cell_type":"code","source":"lst = []\nfor _ in [0,1,2]:\n    k = r,g,b\n    h = 'rgb'\n    for i in k[_]:\n        for j in i:\n            if j not in lst: lst.append(j)\n    print(f'Значения в канале {h[_]}: {lst}')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:45:47.892482Z","iopub.execute_input":"2024-05-21T11:45:47.892973Z","iopub.status.idle":"2024-05-21T11:45:47.956552Z","shell.execute_reply.started":"2024-05-21T11:45:47.892941Z","shell.execute_reply":"2024-05-21T11:45:47.955131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Создаем список annots с массивами array аннотаций:**","metadata":{}},{"cell_type":"code","source":"# если на картинке нет людей, т.е. цифр 255 в слое 'r', то картинка пропускается, иначе добавляется\nfrom tqdm import tqdm\nannots = []\nfor ann in tqdm(mask_files):\n    _ann = cv2.imread(ann, 1).transpose(2,0,1)#/255\n    r,g,b = _ann\n    if 255 not in np.unique(r): continue # отсеиваем маски в которых нет аномалии\n    annots.append(ann)\nprint(f'Количество элементов: {len(annots)}')\nprint(f'Количество уникальных элементов: {len(set(annots))}')\nprint(f'Пример содержимого: {annots[:1]}')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:46:26.877645Z","iopub.execute_input":"2024-05-21T11:46:26.878049Z","iopub.status.idle":"2024-05-21T11:46:37.149334Z","shell.execute_reply.started":"2024-05-21T11:46:26.878019Z","shell.execute_reply":"2024-05-21T11:46:37.147991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. train_test_split**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n_annots = [i.split('/')[-1].replace('.tif','') for i in annots] # список масок без расширений: ['ADE_train_00016892', 'ADE_train_00004335', ....\n# _annots = stems(annots)\ntrn_items, val_items = train_test_split(_annots, test_size=0.2, random_state=2)\nprint(f'Длина trn_items: {len(trn_items)}')\nprint(f'Длина val_items: {len(val_items)}')\nprint(f'Пример содержимого trn_items: {trn_items[:1]}')\nprint(f'Пример содержимого val_items: {val_items[:1]}')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:46:40.448518Z","iopub.execute_input":"2024-05-21T11:46:40.448975Z","iopub.status.idle":"2024-05-21T11:46:40.461294Z","shell.execute_reply.started":"2024-05-21T11:46:40.448944Z","shell.execute_reply":"2024-05-21T11:46:40.460050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6. Аугментация**","metadata":{}},{"cell_type":"code","source":"def get_transform(train):\n    image_transforms = []\n    image_transforms.append(T.PILToTensor())\n#     if train:\n#         image_transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(image_transforms)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:01:35.576033Z","iopub.execute_input":"2024-05-21T14:01:35.576448Z","iopub.status.idle":"2024-05-21T14:01:35.583095Z","shell.execute_reply.started":"2024-05-21T14:01:35.576418Z","shell.execute_reply":"2024-05-21T14:01:35.581551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **7. Создаем класс для создания пакетов**","metadata":{}},{"cell_type":"code","source":"# items - список названий файлов-масок без расширения\n# way_pic - список путей до файлов-изображений с расширениями\n# way_ann - список путей до файлов-масок с расширениями\n\nclass MasksDataset(Dataset):\n    def __init__(self, items, transforms, way_pic, way_ann):#, N): \n        self.items = items # список картинок: ['ADE_train_00016892', 'ADE_train_00004335', ....\n        self.transforms = transforms\n        self.way_pic = way_pic\n        self.way_ann = way_ann\n#         self.N = len(items) # ограничивает количество картинок для рассмотрения\n    def get_mask(self, path):\n        an = cv2.imread(path, 1).transpose(-1,0,1)/255 # делаем из картинки array и меняем каналы метстами, стало: (3,450,640)\n        r,g,b = an # вытаскиваем отдельные каналы\n        nzs = np.nonzero(r==1.0) # вытаскивает координатытточек людей, т.е. значений 4: (array([319, 319, 319, ..., 500, 500, 500]), array([139, 140, 141, ..., 429, 430, 613]))\n        instances = np.unique(g[nzs]) # в слое r находим коорд людей, в слое g для каждого человека свои цифры, короче здесь список цифр, которые обозначают людей\n        masks = np.zeros((len(instances), *r.shape), float) # (39, 512, 683),т.е. на картинке 39 людей, здесь созданы 39 нулевых масок\n        for ix,_id in enumerate(instances):\n            masks[ix] = g==_id # маска состоит из 0 и 1, 0 - это черный фон, 1 - это человечек\n        return masks # список масок на которых по одному человечку, обозначенному единичкой\n    def __getitem__(self, ix):\n        _id = self.items[ix] # название картинки , здесь 'TCGA_DU_7010_19860307_43_mask'\n        for i in self.way_pic:\n            if _id[:-5] in i:\n#                 print(i)\n                img_path = i\n        for i in self.way_ann:\n            if _id[:-5] in i:\n#                 print(i)\n                mask_path = i\n        masks = self.get_mask(mask_path) # это список масок людей на этой картинке, каждаякартинка\n        \n        obj_ids = np.arange(1, len(masks)+1) # array список по кол-ву картинок array([ 1,  2,  3,  4,  5,..., 36, 37, 38, 39])\n        img = Image.open(img_path).convert(\"RGB\")\n        num_objs = len(obj_ids) # количество масок (39)\n        boxes = []\n        for i in range(num_objs):\n            obj_pixels = np.where(masks[i]) # (array([472, 472, 472, 472,..., 478, 478, 478]), array([343, 344, 345, 346,..., 345, 346, 347, 348]))\n                                            # т.к. человечки это единички, то это True, здесь np.where возвращает координаты всех пикселей True (человечков)\n            xmin = np.min(obj_pixels[1])\n            xmax = np.max(obj_pixels[1])\n            ymin = np.min(obj_pixels[0])\n            ymax = np.max(obj_pixels[0])\n            if (((xmax-xmin)<=10) | (ymax-ymin)<=10): # я так понимаю,что этот цикл для избежания ошибки =0\n                xmax = xmin+10\n                ymax = ymin+10\n            boxes.append([xmin, ymin, xmax, ymax])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32) # список коробок переводим в тензор\n        labels = torch.ones((num_objs,), dtype=torch.int64) # создаем список из единичек длиной по числу людей на картинке tensor([1, 1, 1, 1, ..... 1, 1, 1, 1])\n        masks = torch.as_tensor(masks, dtype=torch.uint8) # список масок переводим в тензор\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) #площадь: tensor([100., 100., 154., 143., 407., 456., 396., 256.,  90., 119., 180., 162.,\n        # 200., 115., 243., 225., 196., 455., 147., 208., 216., 410.,  95., 115.,90., 147., 100., 132., 160.,  52., 216., 225., 312.,  80., 100., 100., 100., 100., 100.])\n        # это площади всех коробок людей, здесь 39 людей и выходит столько же коробок\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # создаем список из ноликов длиной по числу людей на картинке tensor([0, 0, 0, 0, ..... 0, 0, 0, 0])\n        image_id = torch.tensor([ix]) # tensor([0]) - индекс-тензор картинки\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n#         print(target)\n        if self.transforms is not None:\n            img = self.transforms(img)\n        if (img.dtype == torch.float32) or (img.dtype == torch.uint8) :\n            img = img/255.\n        return img, target\n    def __len__(self):\n        return len(self.items)\n    def choose(self):\n        return self[randint(len(self))]\n    def collate_fn(self, batch):\n        return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:51:42.855420Z","iopub.execute_input":"2024-05-21T16:51:42.855936Z","iopub.status.idle":"2024-05-21T16:51:42.882898Z","shell.execute_reply.started":"2024-05-21T16:51:42.855900Z","shell.execute_reply":"2024-05-21T16:51:42.880940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = MasksDataset(trn_items, get_transform(train=False),brain_scans,mask_files)#, N=len(trn_items))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:51:46.190864Z","iopub.execute_input":"2024-05-21T16:51:46.191323Z","iopub.status.idle":"2024-05-21T16:51:46.198399Z","shell.execute_reply.started":"2024-05-21T16:51:46.191289Z","shell.execute_reply":"2024-05-21T16:51:46.197049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in dataset:\n    print(i[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:51:48.252853Z","iopub.execute_input":"2024-05-21T16:51:48.253361Z","iopub.status.idle":"2024-05-21T16:51:48.276764Z","shell.execute_reply.started":"2024-05-21T16:51:48.253312Z","shell.execute_reply":"2024-05-21T16:51:48.275736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = MasksDataset(trn_items, get_transform(train=False), brain_scans, mask_files)\ndata_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=dataset.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:51:50.542426Z","iopub.execute_input":"2024-05-21T16:51:50.543467Z","iopub.status.idle":"2024-05-21T16:51:50.549950Z","shell.execute_reply.started":"2024-05-21T16:51:50.543428Z","shell.execute_reply":"2024-05-21T16:51:50.548328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in data_loader:\n    print(i[1])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:53:03.678653Z","iopub.execute_input":"2024-05-21T16:53:03.679105Z","iopub.status.idle":"2024-05-21T16:53:03.727297Z","shell.execute_reply.started":"2024-05-21T16:53:03.679069Z","shell.execute_reply":"2024-05-21T16:53:03.726112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **7. Функции левые для обучения**","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install coco-eval\n!pip install coco-utils","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:04:32.258659Z","iopub.execute_input":"2024-05-21T17:04:32.259086Z","iopub.status.idle":"2024-05-21T17:07:09.357941Z","shell.execute_reply.started":"2024-05-21T17:04:32.259056Z","shell.execute_reply":"2024-05-21T17:07:09.354610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport sys\nimport time\n\nimport torch\nimport torchvision.models.detection.mask_rcnn\nimport utils\nfrom coco_eval import CocoEvaluator\nfrom coco_utils import get_coco_api_from_dataset\n\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n    header = f\"Epoch: [{epoch}]\"\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1.0 / 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n        )\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n        with torch.cuda.amp.autocast(enabled=scaler is not None):\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping training\")\n            print(loss_dict_reduced)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        if scaler is not None:\n            scaler.scale(losses).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            losses.backward()\n            optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n    return metric_logger\n\n\ndef _get_iou_types(model):\n    model_without_ddp = model\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model_without_ddp = model.module\n    iou_types = [\"bbox\"]\n    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n        iou_types.append(\"segm\")\n    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n        iou_types.append(\"keypoints\")\n    return iou_types\n\n\n@torch.inference_mode()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    # FIXME remove this and make paste_masks_in_image run on the GPU\n    torch.set_num_threads(1)\n    cpu_device = torch.device(\"cpu\")\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = \"Test:\"\n\n    coco = get_coco_api_from_dataset(data_loader.dataset)\n    iou_types = _get_iou_types(model)\n    coco_evaluator = CocoEvaluator(coco, iou_types)\n\n    for images, targets in metric_logger.log_every(data_loader, 100, header):\n        images = list(img.to(device) for img in images)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        model_time = time.time()\n        outputs = model(images)\n\n        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n        evaluator_time = time.time()\n        coco_evaluator.update(res)\n        evaluator_time = time.time() - evaluator_time\n        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    coco_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    coco_evaluator.accumulate()\n    coco_evaluator.summarize()\n    torch.set_num_threads(n_threads)\n    return coco_evaluator","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:27:42.572098Z","iopub.execute_input":"2024-05-21T17:27:42.572912Z","iopub.status.idle":"2024-05-21T17:27:42.721376Z","shell.execute_reply.started":"2024-05-21T17:27:42.572859Z","shell.execute_reply":"2024-05-21T17:27:42.719667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n    header = f\"Epoch: [{epoch}]\"\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1.0 / 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n        )\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n        with torch.cuda.amp.autocast(enabled=scaler is not None):\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping training\")\n            print(loss_dict_reduced)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        if scaler is not None:\n            scaler.scale(losses).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            losses.backward()\n            optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n    return metric_logger","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:28:30.139699Z","iopub.execute_input":"2024-05-21T17:28:30.140832Z","iopub.status.idle":"2024-05-21T17:28:30.160613Z","shell.execute_reply.started":"2024-05-21T17:28:30.140789Z","shell.execute_reply":"2024-05-21T17:28:30.158945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **8. Модель, оптимайзер, lr и др.**","metadata":{}},{"cell_type":"code","source":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # получить количество входных признаков для классификатора\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,num_classes)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:27:54.960231Z","iopub.execute_input":"2024-05-21T17:27:54.960815Z","iopub.status.idle":"2024-05-21T17:27:54.970365Z","shell.execute_reply.started":"2024-05-21T17:27:54.960773Z","shell.execute_reply":"2024-05-21T17:27:54.968624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model_instance_segmentation(2).to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:27:58.472143Z","iopub.execute_input":"2024-05-21T17:27:58.472648Z","iopub.status.idle":"2024-05-21T17:28:01.708430Z","shell.execute_reply.started":"2024-05-21T17:27:58.472610Z","shell.execute_reply":"2024-05-21T17:28:01.706908Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 2\nmodel = get_model_instance_segmentation(num_classes).to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                step_size=3,\n                                                gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:28:05.452402Z","iopub.execute_input":"2024-05-21T17:28:05.454105Z","iopub.status.idle":"2024-05-21T17:28:06.703671Z","shell.execute_reply.started":"2024-05-21T17:28:05.454056Z","shell.execute_reply":"2024-05-21T17:28:06.702337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 4\n\ntrn_history = []\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    res = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    trn_history.append(res)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    res = evaluate(model, data_loader_test, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:28:34.214768Z","iopub.execute_input":"2024-05-21T17:28:34.215255Z","iopub.status.idle":"2024-05-21T17:28:34.338314Z","shell.execute_reply.started":"2024-05-21T17:28:34.215218Z","shell.execute_reply":"2024-05-21T17:28:34.336304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **дальше еще не рабочее**","metadata":{}},{"cell_type":"code","source":"!pip install utils\nimport utils","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:32:52.263790Z","iopub.execute_input":"2024-05-21T15:32:52.264285Z","iopub.status.idle":"2024-05-21T15:33:11.566484Z","shell.execute_reply.started":"2024-05-21T15:32:52.264252Z","shell.execute_reply":"2024-05-21T15:33:11.564725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch_snippets import *\n# from torch_snippets.inspector import inspect\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n# from engine import train_one_epoch, evaluate\nimport utils\n# import transforms as T","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:34:20.980058Z","iopub.execute_input":"2024-05-21T15:34:20.981759Z","iopub.status.idle":"2024-05-21T15:34:20.989271Z","shell.execute_reply.started":"2024-05-21T15:34:20.981696Z","shell.execute_reply":"2024-05-21T15:34:20.987653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from detection import utils","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:35:56.671549Z","iopub.execute_input":"2024-05-21T15:35:56.672089Z","iopub.status.idle":"2024-05-21T15:35:56.721103Z","shell.execute_reply.started":"2024-05-21T15:35:56.672051Z","shell.execute_reply":"2024-05-21T15:35:56.719162Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = MasksDataset(trn_items, get_transform(train=False), brain_scans, mask_files)\ndataset_test = MasksDataset(val_items, get_transform(train=False), brain_scans, mask_files)\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=0,\n    collate_fn=dataset.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n    collate_fn=dataset_test.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:45:16.085074Z","iopub.execute_input":"2024-05-21T15:45:16.085843Z","iopub.status.idle":"2024-05-21T15:45:16.094007Z","shell.execute_reply.started":"2024-05-21T15:45:16.085803Z","shell.execute_reply":"2024-05-21T15:45:16.092329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in data_loader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:45:49.212625Z","iopub.execute_input":"2024-05-21T15:45:49.213117Z","iopub.status.idle":"2024-05-21T15:45:49.514755Z","shell.execute_reply.started":"2024-05-21T15:45:49.213085Z","shell.execute_reply":"2024-05-21T15:45:49.512545Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = '/kaggle/input/lgg-mri-segmentation/lgg-mri-segmentation/kaggle_3m/TCGA_DU_7306_19930512/TCGA_DU_7306_19930512_35_mask.tif'\nst.split('/')[-1].replace('_mask','')[:-4]","metadata":{"execution":{"iopub.status.busy":"2024-05-20T09:16:28.237272Z","iopub.execute_input":"2024-05-20T09:16:28.237657Z","iopub.status.idle":"2024-05-20T09:16:28.245457Z","shell.execute_reply.started":"2024-05-20T09:16:28.237630Z","shell.execute_reply":"2024-05-20T09:16:28.244130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'/kaggle/input/lgg-mri-segmentation/lgg-mri-segmentation/kaggle_3m/TCGA_DU_7306_19930512/TCGA_DU_7306_19930512_35_mask.tif'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}